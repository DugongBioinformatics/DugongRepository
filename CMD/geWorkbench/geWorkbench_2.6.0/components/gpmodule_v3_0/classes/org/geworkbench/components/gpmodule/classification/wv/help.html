<html>
<body>

<p class=MsoNormal style='mso-margin-top-alt:auto;mso-margin-bottom-alt:auto;
tab-stops:center 693.0pt;mso-layout-grid-align:none;text-autospace:none'><span
style='font-family:Arial'>The Weighted Voting algorithm makes a weighted linear
combination of relevant &#8220;marker&#8221; or &#8220;informative&#8221;
features obtained in the training set to provide a classification scheme for
new samples. Target classes (classes 0 and 1) can be for example defined based
on a phenotype such as morphological class or treatment outcome. The selection
of classifier input features (marker features) is accomplished either by
computing a signal-to-noise statistic <span class=SpellE>S<span
style='font-size:8.0pt'>x</span></span></span><span style='font-size:8.0pt;
font-family:Arial'> </span><span style='font-family:Arial'>= (&#956;</span><span
style='font-size:8.0pt;font-family:Arial'>0 </span><span style='font-family:
Arial'>- &#956;</span><span style='font-size:8.0pt;font-family:Arial'>1</span><span
style='font-family:Arial'>)/( &#963;</span><span style='font-size:8.0pt;
font-family:Arial'>0 </span><span style='font-family:Arial'>+ &#963;</span><span
style='font-size:8.0pt;font-family:Arial'>1</span><span style='font-family:
Arial'>) where &#956;</span><span style='font-size:8.0pt;font-family:Arial'>0 </span><span
style='font-family:Arial'>is the mean of class 0 and &#963;</span><span
style='font-size:8.0pt;font-family:Arial'>0 </span><span style='font-family:
Arial'>is the standard deviation of class 0 or by reading in a list of user
provided features. The class predictor is uniquely defined by the initial set
of samples and markers. In addition to computing <span class=SpellE><i>S</i><i><span
style='font-size:8.0pt'>x</span></i></span>, the algorithm also finds the
decision boundaries (half way) between the class means: <span class=SpellE>B<span
style='font-size:8.0pt'>x</span></span></span><span style='font-size:8.0pt;
font-family:Arial'> </span><span style='font-family:Arial'>= (&#956;</span><span
style='font-size:8.0pt;font-family:Arial'>0 </span><span style='font-family:
Arial'>+ &#956;</span><span style='font-size:8.0pt;font-family:Arial'>1</span><span
style='font-family:Arial'>)/2 for each feature x. To predict the class of a
test sample y, each feature x in the feature set casts a vote: <span
class=SpellE>V<span style='font-size:8.0pt'>x</span></span></span><span
style='font-size:8.0pt;font-family:Arial'> </span><span style='font-family:
Arial'>= <span class=SpellE>S<span style='font-size:8.0pt'>x</span></span></span><span
style='font-size:8.0pt;font-family:Arial'> </span><span style='font-family:
Arial'>(<span class=SpellE>G<span style='font-size:8.0pt'>xy</span></span></span><span
style='font-size:8.0pt;font-family:Arial'> </span><span style='font-family:
Arial'>&#8211; <span class=SpellE>B<span style='font-size:8.0pt'>x</span></span>)
and the final vote for class 0 or 1 is <span class=GramE>sign(</span><span
class=SpellE>S<span style='font-size:8.0pt'>x</span></span></span><span
style='font-size:8.0pt;font-family:Arial'> </span><span class=SpellE><span
style='font-family:Arial'>V</span><span style='font-size:8.0pt;font-family:
Arial'>x</span></span><span style='font-family:Arial'>). The strength or
confidence in the prediction of the winning class is (<span class=SpellE>V<span
style='font-size:8.0pt'>win</span></span></span><span style='font-size:8.0pt;
font-family:Arial'> </span><span style='font-family:Arial'>- <span
class=SpellE>V<span style='font-size:8.0pt'>lose</span></span>)<span
class=GramE>/(</span><span class=SpellE>V<span style='font-size:8.0pt'>win</span></span></span><span
style='font-size:8.0pt;font-family:Arial'> </span><span style='font-family:
Arial'>+ <span class=SpellE>V<span style='font-size:8.0pt'>lose</span></span>)
(i.e., the relative margin of victory for the vote). Notice that this algorithm
is quite similar to Na&iuml;ve <span class=SpellE>Bayes</span> (see the
appendix in <span class=SpellE>Slonim</span> et al. 2000).<u1:p></u1:p></span></p>

<p class=MsoNormal style='mso-margin-top-alt:auto;mso-margin-bottom-alt:auto;
tab-stops:center 693.0pt;mso-layout-grid-align:none;text-autospace:none'><o:p>&nbsp;</o:p></p>

<p class=MsoNormal style='mso-margin-top-alt:auto;mso-margin-bottom-alt:auto;
mso-layout-grid-align:none;text-autospace:none'><b><span style='font-family:
Arial'>References:<u1:p></u1:p></span></b></p>

<ul type=disc>
 <li class=MsoNormal style='mso-margin-top-alt:auto;mso-margin-bottom-alt:auto;
     mso-list:l0 level1 lfo3;tab-stops:list .5in;mso-layout-grid-align:none;
     text-autospace:none'><span class=SpellE><span style='font-family:Arial'>Golub</span></span><span
     style='font-family:Arial'> T.R., <span class=SpellE>Slonim</span> D.K., et
     al. &#8220;Molecular Classification of Cancer: Class Discovery and Class
     Prediction by Gene Expression Monitoring,&#8221; Science, 531- 537 (1999).<u1:p></u1:p></span></li>
 <li class=MsoNormal style='mso-margin-top-alt:auto;mso-margin-bottom-alt:auto;
     mso-list:l0 level1 lfo3;tab-stops:list .5in;mso-layout-grid-align:none;
     text-autospace:none'><span class=SpellE><span style='font-family:Arial'>Slonim</span></span><span
     style='font-family:Arial'>, D.K., Tamayo, P., <span class=SpellE>Mesirov</span>,
     J.P., <span class=SpellE>Golub</span>, T.R., Lander, E.S. (2000) Class
     prediction and discovery using gene expression data. In Proceedings of the
     Fourth Annual International Conference on Computational Molecular Biology
     (RECOMB) 2000. ACM Press, <st1:place w:st="on"><st1:State w:st="on">New
       York</st1:State></st1:place>, pp. 263&#8211;272.<u1:p></u1:p></span></li>
</ul>

</body>

</html>
